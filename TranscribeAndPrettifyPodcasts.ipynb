{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup\n\nImporting dependencies, downloading openai-whisper, sentence-transformers, ffmpeg...","metadata":{}},{"cell_type":"code","source":"!pip install --quiet -U pip\n!pip install --quiet -U openai-whisper\n!pip install feedparser\n!pip install --quiet -U sentence-transformers\n!apt-get update && apt install -y ffmpeg","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall whisper\n!pip install --force-reinstall openai-whisper==20230124","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.signal import argrelextrema\nimport math\n\nimport whisper\n\nimport feedparser\nimport urllib.request\n\n### transcription model ###\nmodel_size = \"large-v2\"\n\nprint('Loading transcription model...')\n# Run on GPU with FP16\nmodel = whisper.load_model(model_size) #, device=\"cuda\")\n# or run on GPU with INT8\n# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n# or run on CPU with INT8\n# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\nprint('Done.')\n\n### paragraph model ###\nprint('Loading word embedding model...')\nsentencemodel = SentenceTransformer('all-mpnet-base-v2')\nprint('Done.')\n\ndef rev_sigmoid(x:float)->float:\n    return (1 / (1 + math.exp(0.5*x)))\n    \ndef activate_similarities(similarities:np.array, p_size=10)->np.array:\n        \"\"\" Function returns list of weighted sums of activated sentence similarities\n        Args:\n            similarities (numpy array): it should square matrix where each sentence corresponds to another with cosine similarity\n            p_size (int): number of sentences are used to calculate weighted sum \n        Returns:\n            list: list of weighted sums\n        \"\"\"\n        # To create weights for sigmoid function we first have to create space. P_size will determine number of sentences used and the size of weights vector.\n        x = np.linspace(-10,10,p_size)\n        # Then we need to apply activation function to the created space\n        y = np.vectorize(rev_sigmoid) \n        # Because we only apply activation to p_size number of sentences we have to add zeros to neglect the effect of every additional sentence and to match the length ofvector we will multiply\n        activation_weights = np.pad(y(x),(0,similarities.shape[0]-p_size))\n        ### 1. Take each diagonal to the right of the main diagonal\n        diagonals = [similarities.diagonal(each) for each in range(0,similarities.shape[0])]\n        ### 2. Pad each diagonal by zeros at the end. Because each diagonal is different length we should pad it with zeros at the end\n        diagonals = [np.pad(each, (0,similarities.shape[0]-len(each))) for each in diagonals]\n        ### 3. Stack those diagonals into new matrix\n        diagonals = np.stack(diagonals)\n        ### 4. Apply activation weights to each row. Multiply similarities with our activation.\n        diagonals = diagonals * activation_weights.reshape(-1,1)\n        ### 5. Calculate the weighted sum of activated similarities\n        activated_similarities = np.sum(diagonals, axis=0)\n        return activated_similarities\n\ndef paragraphise(bigstring):\n    sentences = bigstring.split('. ')\n    \n    embeddings = sentencemodel.encode(sentences, show_progress_bar=False)\n    similarities = cosine_similarity(embeddings)\n    activated_similarities = activate_similarities(similarities, p_size=5)\n    minimas = argrelextrema(activated_similarities, np.less, order=2) #order parameter controls how frequent should be splits. I would not reccomend changing this parameter.\n\n    split_points = [each for each in minimas[0]]\n    text = ''\n    for num,each in enumerate(sentences):\n        if num in split_points:\n            text+=f'\\n\\n {each}. '\n        else:\n            text+=f'{each}. '\n    return text\n\n### other functions ### \ndef zfill_alternative(x,l=2): return x if len(x) >= l else '0'*(l-len(x))+x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transcribe and Prettify Podcast from RSS feed\n\nThe very clever idea for making paragraphs was taken from [this notebook](https://github.com/poloniki/quint/blob/master/notebooks/Chunking%20text%20into%20paragraphs.ipynb).\n\nThe example podcast used in the notebook is a Russian book podcast [Knizhnyy Bazar](https://tehnikarechi.studio/podcasts/knizhnyy-bazar).\n\n","metadata":{}},{"cell_type":"code","source":"import feedparser\nimport urllib.request\nNewsFeed = feedparser.parse(\"https://tehnikarechi.studio/api/rss/podcasts/knizhnyy-bazar\")\nentry = NewsFeed.entries[0]\n\nprint(entry.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir knizhnyy-bazar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NewsFeed = feedparser.parse(\"https://tehnikarechi.studio/api/rss/podcasts/knizhnyy-bazar\")\n\ndl_dir = 'knizhnyy-bazar'\n\nfailsforsomereason = []\n\narr = os.listdir('/kaggle/working/'+dl_dir)\n\nfor entry in tqdm(NewsFeed.entries):\n    notdone = True\n    while notdone:\n        try:\n            # Get the title and discard if already done.\n            vid_title = re.sub(r'[^A-Za-zЁёА-я0-9— ]+', '', entry['title'].replace(u'\\xa0', u' '))[:130]\n\n            if (vid_title in [x[:-4] for x in arr] or \n                vid_title in failsforsomereason):\n                notdone = False\n                continue\n                \n            # Download audio file. \n            opener = urllib.request.build_opener()\n            opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n            urllib.request.install_opener(opener)\n            urllib.request.urlretrieve(entry['links'][1]['href'], \"audio.mp3\")\n            print(vid_title)\n\n            # Transcribe. \n            transcription = model.transcribe(\"audio.mp3\",  language=\"ru\")\n            \n            # Paragraphise the string. \n            prettytext = paragraphise(transcription['text'])\n\n            # Finally, save along with some metadata. \n            with open(dl_dir+'/'+vid_title+'.txt','w+',encoding='utf-8') as myfile:\n                myfile.write('---')\n                myfile.write('\\ntitle: '+vid_title)\n                myfile.write('\\nauthor: '+entry['author'])\n                myfile.write('\\npublished: '+entry['published'])\n                myfile.write('\\ntags: '+str([ a['term'] for a in entry['tags']]))\n                myfile.write('\\n---\\n\\n')\n                myfile.write(prettytext)\n            \n            print('Done')\n            notdone = False\n        except Exception as e:\n            continue","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -zcvf knizhnyy-bazar.tar.gz knizhnyy-bazar/*.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'knizhnyy-bazar.tar.gz')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}